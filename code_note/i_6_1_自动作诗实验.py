# -*-coding:utf-8-*-

"""
（1）古诗清洗、过滤较长或较短古诗、过滤既非五言也非七言古诗、为每个字生成唯一的数字ID、每首古诗用数字ID表示。
（2）两层RNN模型，采用LSTM模型。
（3）训练LSTM模型。
（4）生成古诗，随机取一个汉字，根据该汉字生成一首古诗。
"""
from i_6_1_自动作诗实验.generate_poetry import Poetry


# 清洗数据 generate_poetry.py
"""
p = Poetry()
# 第一首诗数字表示
print(p.poetry_vectors[0])

# 根据ID查看对应的汉字
print(p.id_to_word[1101])

# 根据汉字查看对应的数字
print(p.word_to_id[u'寒'])

# 查看x_batch，y_batch的输出
x_batch, y_batch = p.next_batch(1)
print(x_batch)
print(y_batch)
"""

# LSTM模型学习：poetry_model.py
"""
import numpy as np
np.random.seed(100)
# 在模型训练过程中，需要对每个字进行向量化，embedding的作用是按照inputs顺序返回embedding中的对应行
embedding = np.random.random([100, 10])
print(embedding.shape)
print(embedding)
inputs = np.array([1, 2, 3, 4])
print(embedding[inputs])
"""
# [[0.89132195 0.20920212 0.18532822 0.10837689 0.21969749 0.97862378
#   0.81168315 0.17194101 0.81622475 0.27407375]
#  [0.43170418 0.94002982 0.81764938 0.33611195 0.17541045 0.37283205
#   0.00568851 0.25242635 0.79566251 0.01525497]
#  [0.59884338 0.60380454 0.10514769 0.38194344 0.03647606 0.89041156
#   0.98092086 0.05994199 0.89054594 0.5769015 ]
#  [0.74247969 0.63018394 0.58184219 0.02043913 0.21002658 0.54468488
#   0.76911517 0.25069523 0.28589569 0.85239509]]

# [5.43404942e-01 2.78369385e-01 4.24517591e-01 8.44776132e-01
#   4.71885619e-03 1.21569121e-01 6.70749085e-01 8.25852755e-01
#   1.36706590e-01 5.75093329e-01]
#  [8.91321954e-01 2.09202122e-01 1.85328220e-01 1.08376890e-01
#   2.19697493e-01 9.78623785e-01 8.11683149e-01 1.71941013e-01
#   8.16224749e-01 2.74073747e-01]
#  [4.31704184e-01 9.40029820e-01 8.17649379e-01 3.36111950e-01
#   1.75410454e-01 3.72832046e-01 5.68850735e-03 2.52426353e-01
#   7.95662508e-01 1.52549712e-02]
#  [5.98843377e-01 6.03804539e-01 1.05147685e-01 3.81943445e-01
#   3.64760566e-02 8.90411563e-01 9.80920857e-01 5.99419888e-02
#   8.90545945e-01 5.76901499e-01]


# 训练LSTM模型：poetry_model.py
"""
每批次采用50首诗训练，训练40 000次后，损失函数基本保持不变，GPU大概需要2个小时。
当然也可以调整循环次数，节省训练时间，或者直接下载已经训练好的模型。
"""


