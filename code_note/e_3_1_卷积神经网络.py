# -*-coding:utf-8-*-
import numpy as np

"""
卷积神经网络（Convolutional Neural Network，CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward NeuralNetwork），是深度学习的代表算法之一。
卷积神经网络具有表征学习（Representation Learning）能力，能够按其阶层结构对输入信息进行平移不变分类（Shift-invariant Classification），
因此也被称为平移不变人工神经网络（Shift-Invariant Artificial NeuralNetwork，SIANN）
"""
"""
典型的卷积神经网络结构主要分为输入层、卷积层、池化层、全连接层、分类层等，
（1）输入层（Input Layer）。
（2）卷积层（Convolution Layer）。卷积层试图将神经网络中的每小块进行更加深入地分析从而获得抽象度更高的特征。
（3）池化层（Pooling Layer）。池化层可以进一步缩小最后全连接层中节点的个数，从而达到减小整个神经网络参数的目的。
（4）全连接层（Full Connection Layer）。将卷积层和池化层看作自动图像特征ᨀ取的过程，在特征ᨀ取之后，仍要用全连接层来完成分类问题。
（5）Softmax层。用于分类问题，通过Softmax层可以得到当前输出属于不同种类的概率分布情况。
"""
"""
Softmax函数，又称归一化指数函数，是对数概率回归在 个不同值上的推广，
"""


# softmax函数
def softmax(x):
    exp_x = np.exp(x)
    return exp_x / np.sum(exp_x)


# 卷积层
# 最重要的部分就是过滤器（Filter）或者叫作内核（Kernel）
"""
过滤器可以将当前神经网络的一个子节点矩阵转化为下一层神经网络的一个单位节点矩阵。
单位节点矩阵就是长和宽都是1，但深度不限的节点矩阵。
卷积层计算方式：
W2 = (W1 - F + 2P)/S + 1
W2表示输出特征图的宽， W1表示输入图像的宽， F表示过滤器的宽， P是填充零的圈数， S是步幅。
长和宽等价，所以图像的长也可以用式计算
"""

# 池化层
# 池化层既可以加快计算速度也可以有效地防止过拟合问题
"""
池化方式是最大池化（Max Pooling）和平均池化（Average Pooling）。
池化层的过滤器只影响一个深度上的节点，即主要减小矩阵的长和宽，不减少矩阵的深度
"""

# 全连接层
# 几个卷积和池化层之后通常有一个或多个全连接层（Fully Connected Layers，FCL），旨在执行对原始图像的高级抽象。
"""
全连接层在整个卷积神经网络中起到“分类器”的作用。
如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间，
则全连接层起到将学到的“分布式特征表示”映射到样本标记空间的作用。
全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），
"""

"""
典型卷积神经网络
AlexNet
VGGNet-16

"""








