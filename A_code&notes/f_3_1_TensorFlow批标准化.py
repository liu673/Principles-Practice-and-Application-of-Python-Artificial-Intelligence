# -*-coding:utf-8-*-


"""
批标准化（Batch Normalization，BN）是为了克服神经网络层数加深导致难以训练而诞生的。
随着神经网络的深度加深，训练会越来越困难，收敛速度会很慢，常常会导致梯度消失问题

梯度消失问题是在神经网络中，当前隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，分类准确率反而下降了，这种现象叫梯度消失问题。

协变量转移（Covariate Shift）是指当训练集的样本数据和目标样本集分布不一致时，训练得到的模型无法很好地泛化
（泛化通俗地讲就是指学习到的模型对位置数据的预知能力）。
"""

"""
批标准化一般用在非线性映射（激活函数）之前，对 x=Wu + b做规范化，使结果（输出信号各个维度）的均值为0，方差为1。让每一层
的输入有一个稳定的分布会有利于网络的训练。批标准化通过规范化让激活函数分布在线性区间，结果就是加大了梯度，让模型更加大胆地进行梯度下降。

批标准化的优点：
（1）加大探索的步长，从而加快收敛的速度
（2）更容易跳出局部最小值
（3）破坏原来的数据分布，在一定程度上缓解过拟合
"""





